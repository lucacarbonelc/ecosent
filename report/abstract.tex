Sentiment is central to many studies of communication science, from negativity and polarization in political communication to analyzing product reviews and social media comments in other sub-fields. 
This study provides an exhaustive comparison of sentiment analysis methods, using a 
validation set of Dutch economic headlines
to compare the performance of manual annotation, crowd coding, numerous 
dictionaries and machine learning using both traditional and deep learning algorithms. 
The three main conclusions of this article are that: 
(1) The best performance is still attained with trained human or crowd coding;
(2) None of the used dictionaries come close to acceptable levels of validity; and 
(3) machine learning, especially deep learning, substantially outperforms dictionary-based methods but falls short of human performance.
From these findings, we stress the importance of always validating automatic text analysis 
methods before usage.
Moreover, we provide a recommended step-by-step approach for (automated) text analysis projects to ensure both efficiency and validity. 
